# -*- coding: utf-8 -*-
"""Shoe Sandal Boot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A5Ai4kCLBuJhMD3FLsn6MI246V3somKw

Proyek Image Classification: **Shoe vs Sandal vs Boot Dataset**
- Nama:**ERIKA BUDIARTI**
- Email: erika.analytic@gmail.com
- Id Dicoding:erika_budiarti

# **Menghubungkan KAGGLE dan Google Colaboratory**
"""

!pip install kaggle

from google.colab import files

# Upload the kaggle.json file
uploaded = files.upload()

!mkdir -p /content/gdrive/My\ Drive/Kaggle
!mv kaggle.json /content/gdrive/My\ Drive/Kaggle/kaggle.json

from google.colab import drive
drive.mount('/content/gdrive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive/Kaggle"

!kaggle datasets download -d hasibalmuzdadid/shoe-vs-sandal-vs-boot-dataset-15k-images

!unzip shoe-vs-sandal-vs-boot-dataset-15k-images.zip

"""# **Import Library**"""

import tensorflow as tf
print(tf.__version__)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.regularizers import l1, l2, l1_l2

import numpy as np
import matplotlib.pyplot as plt

import random
from PIL import Image

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

import pathlib

"""# **Split Folder**"""

# Menyimpan dataset gambar dalam directory "base_dir"
base_dir = '/content/Shoe vs Sandal vs Boot Dataset'

# Menampilkan jumlah gambar untuk setiap kelas
boot_dir = '/content/Shoe vs Sandal vs Boot Dataset/Boot'
sandal_dir = '/content/Shoe vs Sandal vs Boot Dataset/Sandal'
shoe_dir = '/content/Shoe vs Sandal vs Boot Dataset/Shoe'

boot_images = len(os.listdir(boot_dir))
sandal_images = len(os.listdir(sandal_dir))
shoe_images = len(os.listdir(shoe_dir))

print(f'Number of Boot images: {boot_images}')
print(f'Number of Sandal images: {sandal_images}')
print(f'Number of Shoe images: {shoe_images}')

# Menampilkan resolusi gambar dengan menggunakan sample dari kelas "Sandal"

def display_images_with_different_resolution_sandal(sandal_dir, num_images=2):
    print("Class: Sandal")

    image_files = [os.path.join(sandal_dir, file) for file in os.listdir(sandal_dir) if file.endswith(('.jpg', '.jpeg', '.png', '.gif'))]

    resolution_dict = {}
    for path in image_files:
        image_name = Image.open(path)
        width, height = image_name.size
        resolution = (width, height)
        if resolution not in resolution_dict:
            resolution_dict[resolution] = []
        resolution_dict[resolution].append(path)

    for resolution, paths in resolution_dict.items():
        if len(paths) >= num_images:
            random_images = random.sample(paths, num_images)
            print(f"Resolution: {resolution[0]} x {resolution[1]}")
            for path in random_images:
                print(f"  Image: {path}\n")
        else:
            print(f"Resolution: {resolution[0]} x {resolution[1]}")
            for path in paths:
                print(f"  Image: {path}\n")

display_images_with_different_resolution_sandal(sandal_dir, num_images=2)

# Data validasi 20%, data pelatihan 80%
data_generator = ImageDataGenerator(rescale = 1/255., validation_split = 0.2)

# Menampilkan jumlah data pelatihan dan data validasi
train_dir = '/tmp/images/train'
train_generator = data_generator.flow_from_directory(
    base_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset="training"
)

validation_dir = '/tmp/images/val'
validation_generator = data_generator.flow_from_directory(
       base_dir,
       target_size=(224, 224),
       batch_size=32,
       class_mode='categorical',
       subset="validation",
)

"""# **Pembuatan Model**"""

# Mendefinisikan model
model = tf.keras.models.Sequential([
            tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),
            tf.keras.layers.MaxPooling2D((2, 2), strides=2),
            tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
            tf.keras.layers.MaxPooling2D((2, 2), strides=2),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(128, activation = 'relu'),
            tf.keras.layers.Dense(256, activation = 'relu'),
            tf.keras.layers.Dense(3, activation = 'softmax')
])

# Mendefinisikan Callback
class iCallback(tf.keras.callbacks.Callback):
    def __init__(self, model):
        self.model = model

    def on_epoch_end(self, epoch, logs={}):
        if (logs.get('val_accuracy') >= 0.99 and
            logs.get('accuracy') >= 0.99):
            self.model.stop_training = True

reduce_lr_val_accuracy_callback = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_accuracy', factor=0.2, patience=3, verbose=1, min_lr=1e-6)

reduce_lr_accuracy_callback = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='accuracy', factor=0.2, patience=3, verbose=1, min_lr=1e-6)

callback_model = [iCallback(model), reduce_lr_val_accuracy_callback, reduce_lr_accuracy_callback]

# Mendefinisikan Loss
loss = CategoricalCrossentropy()

# Mendefinisikan Optimizer
optimizer = Adam()

# Kompilasi Model
model.compile(loss=loss,
              optimizer=optimizer,
              metrics=['accuracy'])

"""# **Training Data**"""

history = model.fit(
    train_generator,
    batch_size = 32,
    epochs = 25,
    validation_data = validation_generator,
    verbose = 2,
    callbacks = callback_model
    )

# Data akurasi
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

# Data loss
train_loss = history.history['loss']
val_loss = history.history['val_loss']


fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Plot pertama (Akurasi)
axes[0].plot(train_accuracy, label='Train Accuracy')
axes[0].plot(val_accuracy, label='Validation Accuracy')
axes[0].set_title('Akurasi Model')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend(loc='lower right')

# Plot kedua (Loss)
axes[1].plot(train_loss, label='Train Loss')
axes[1].plot(val_loss, label='Validation Loss')
axes[1].set_title('Loss Model')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend(loc='upper right')

plt.tight_layout()
plt.show()

"""# **Mengubah Model**"""

export_dir = 'saved_model/'
tf.saved_model.save(model, export_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()

tflite_model_file = pathlib.Path('imej.tflite')
tflite_model_file.write_bytes(tflite_model)